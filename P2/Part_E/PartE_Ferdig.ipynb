{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class OwnLogisticRegression():\n",
    "    '''\n",
    "    Description:\n",
    "    ------------\n",
    "    This is the class for the Logistic Regression classification code. \n",
    "    Because we have had some trouble with the GD/SGD code in Part A), this\n",
    "    code is written from scratch to solve this task alone, which is why it \n",
    "    differs a little bit from the first GD/SGD code. The Cost function used \n",
    "    in classification cases, as specified in the report, is the CrossEntropy.\n",
    "    This is also the starting point for calculating the gradients used here.\n",
    "\n",
    "    Initialization:\n",
    "    ---------------\n",
    "        X_train (np.array): Training dataset\n",
    "        X_test (np.array): Testing dataset\n",
    "        Y_train (np.array): Target training dataset\n",
    "        Y_test (np.array): Target testing dataset\n",
    "        learning_rate: Eta value (step size) for the GD/SGD algorithms\n",
    "        n_iterations: Number of epochs\n",
    "        batch_size: Size of mini-batches for SGD \n",
    "        l2_lambda : L2 Regularization parameter, default to 0\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "        train_GD : Gradient Descent algorithm for finding the optimal weights and bias, \n",
    "            equivalent to beta-coefficients and intercept for linear regression. Takes in a l2_lambda\n",
    "            regularization parameter as specified in the task, for Lasso regularization. Default value 0.\n",
    "\n",
    "        train_mini_batch : Stochastic Gradient Descent algorithm for finding the optimal weights and bias, \n",
    "            equivalent to beta-coefficients and intercept for linear regression. Includes mini-batches which\n",
    "            makes it stochastic. Also takes in a l2_lambda regularization parameter as specified in the task, \n",
    "            for L2 regularization. Default value 0.\n",
    "\n",
    "        train : Based on the specifiec method, either GD or SGD, fits the model with the optimal parameters.\n",
    "        predict : The method to predict on some test data, using the trained model.\n",
    "        accuracy : A measure of number of correct classifications after the model has made a prediction/classification.\n",
    "         \n",
    "    '''\n",
    "    def __init__(self, X_train,X_test,Y_train,Y_test, learning_rate, n_iterations, batch_size = 10,l2_lambda = 0):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "        self.batch_size = batch_size\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        \n",
    "    # GRADIENT DESCENT\n",
    "    def train_GD(self):\n",
    "        # Extracting inputs and features from X shape\n",
    "        n_inputs, n_features = self.X_train.shape\n",
    "        # Initialization weights and bias to zero\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iter):\n",
    "            linear_prediction = self.X_train @ self.weights + self.bias\n",
    "            predictions = sigmoid(linear_prediction)\n",
    "\n",
    "            # Gradients\n",
    "            dCdW = 1/n_inputs * (self.X_train.T @ (predictions - self.Y_train)) + self.l2_lambda * self.weights\n",
    "            dCdB = 1/n_inputs * np.sum(predictions - self.Y_train) + self.l2_lambda * self.bias\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.lr * dCdW  \n",
    "            self.bias -= self.lr * dCdB\n",
    "        \n",
    "    # STOCHASTIC GRADIENT DESCENT\n",
    "    def train_mini_batch(self):\n",
    "        n_inputs, n_features = self.X_train.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iter):\n",
    "            # Randomly shuffle the data for each iteration\n",
    "            permutation = np.random.permutation(n_inputs)\n",
    "            X_shuffled = self.X_train[permutation, :]\n",
    "            y_shuffled = self.Y_train[permutation]\n",
    "\n",
    "            for i in range(0, n_inputs, self.batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[i:i+self.batch_size, :]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "\n",
    "                linear_prediction = X_batch @ self.weights + self.bias\n",
    "                predictions = sigmoid(linear_prediction)\n",
    "                            \n",
    "                # Gradients\n",
    "                dCdW = 1/len(X_batch) * (X_batch.T @ (predictions - y_batch)) + self.l2_lambda * self.weights\n",
    "                dCdB = 1/len(X_batch) * np.sum(predictions - y_batch) + self.l2_lambda * self.bias\n",
    "\n",
    "                # Update weights and bias\n",
    "                self.weights -= self.lr * dCdW\n",
    "                self.bias -= self.lr * dCdB\n",
    "\n",
    "\n",
    "    # TRAIN METHOD, TAKES IN METHOD AS INPUT PARAMETER    \n",
    "    def train(self, method='GD'):\n",
    "        if method == 'GD':\n",
    "            self.train_GD()\n",
    "        elif method == 'SGD':\n",
    "            self.train_mini_batch()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid training method. Use 'batch', 'sgd', or 'mini_batch'.\")\n",
    "\n",
    "    # The prediction method\n",
    "    def predict(self):\n",
    "        linear_pred = self.X_test @ self.weights + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        # Setting the threshold for True/False to 0.5\n",
    "        class_predictions = [0 if y_true <=0.5 else 1 for y_true in y_pred ]\n",
    "        return class_predictions\n",
    "\n",
    "    # Method for Accuracy Score\n",
    "    def accuracy(self,y_pred, y_test):\n",
    "        return np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Studying Accuracy versus learning rates and L2 regularization parameter for SGD'''\n",
    "\n",
    "# Setting a random seed, NB: Pay attention to this seed and restarting the kernel when running over!\n",
    "\n",
    "np.random.seed(2023) \n",
    "\n",
    "# Loading Wisconcin Breast Cancer Dataset, and setting up the Design Matrix X, and target Y\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2) # From Sklearn.model_selection\n",
    "\n",
    "# Scaling the data using the StandardScaler from Sklearn.preprocessing\n",
    "Scaler = StandardScaler()\n",
    "X_train = Scaler.fit_transform(X_train)\n",
    "X_test = Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculating accuracies over different learning rates and L2 parameters'''\n",
    "\n",
    "# Defining model parameters\n",
    "etas = np.logspace(-6,-1,10)\n",
    "l2_lambdas = np.logspace(-6,-1,10)\n",
    "n_iterations = 1000\n",
    "acc_scores = np.zeros((len(etas), len(l2_lambdas)))\n",
    "\n",
    "for eta_idx in range(len(etas)):\n",
    "    eta = etas[eta_idx]\n",
    "    for l2 in range(len(l2_lambdas)):  \n",
    "        lam = l2_lambdas[l2]\n",
    "        SGD_Test = OwnLogisticRegression(X_train, X_test, Y_train, Y_test, eta,n_iterations,10,lam)\n",
    "        SGD_Test.train(\"SGD\")\n",
    "        y_pred = SGD_Test.predict()\n",
    "        acc = accuracy_score(Y_test,y_pred)\n",
    "        acc_scores[eta_idx,l2] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Creating a Dataframe with Accuracy-data and plotting a heatmap of the results'''\n",
    "\n",
    "\n",
    "# Setting font-family for Matplotlib to \"Times New Roman\" to match Overleaf' s Latex font.\n",
    "sns.set(font='Times New Roman', style='whitegrid', font_scale=1.2)\n",
    "\n",
    "# Creating the dataframe using Pandas as pd\n",
    "acc_scores_df = pd.DataFrame(acc_scores, index = etas, columns = l2_lambdas)\n",
    "\n",
    "# Plot using Seaborn and Matplotlib\n",
    "title = \"Heatmap of SGD Accuracy for different $\\eta$ and $\\lambda_2$ values \"\n",
    "\n",
    "# Define custom tickmark values\n",
    "x_ticks = np.linspace(0, acc_scores_df.shape[1] - 1, 10, dtype=int)\n",
    "y_ticks = np.linspace(0, acc_scores_df.shape[0] - 1, 10, dtype=int)\n",
    "\n",
    "# Adjusting the figure size and annot font size\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(acc_scores_df, annot=True, cmap=\"crest\",fmt=\".3f\", linewidths=0.5, annot_kws={\"size\": 12}, cbar_kws={'label': 'Accuracy Value'})\n",
    "\n",
    "# Set x-axis and y-axis label font sizes\n",
    "ax.set_xlabel(\"$\\lambda_2$, Regularization Parameter\", fontsize=12)  # Add your X-axis label and adjust font size\n",
    "ax.set_ylabel(\"$\\eta$, Learning Rate\", fontsize=12)  # Add your Y-axis label and adjust font size\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_xticklabels(np.around(acc_scores_df.columns[x_ticks], decimals=7))\n",
    "ax.set_yticklabels(np.around(acc_scores_df.index[y_ticks], decimals=7))\n",
    "\n",
    "# Adjust x and y tick font size\n",
    "ax.tick_params(axis='x', labelsize=10)  # Adjust x-axis tick font size\n",
    "ax.tick_params(axis='y', labelsize=10)  # Adjust y-axis tick font size\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.yticks(rotation=0)   # Rotate y-axis labels for better readability\n",
    "plt.title(title, fontsize=16)\n",
    "\n",
    "# Adjust colorbar font size\n",
    "cax = ax.collections[0].colorbar.ax\n",
    "cax.set_ylabel('Accuracy Value', fontsize=12) \n",
    "cax.tick_params(labelsize=10)  # Adjust colorbar font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Comparing SGD, GD, and Scikit-Learn\n",
    "-----------------------------------\n",
    "    Based on the optimal range of lambdas and etas, from the Heatmap above,\n",
    "    a new comparison (using optimal values) are done with the different methods:\n",
    "    SGD, GD, and Scikit-Learn\n",
    "'''\n",
    "\n",
    "# Specifying parameters, and learning rate array\n",
    "eta = 0.0001\n",
    "lam = 0.01 # from the results above\n",
    "n_iterations = range(1,600+1,1)\n",
    "\n",
    "# Empty arrays to store the accuracies\n",
    "acc_array_SGD = np.zeros(len(n_iterations))\n",
    "acc_array_GD = np.zeros(len(n_iterations))\n",
    "acc_array_Scikit = np.zeros(len(n_iterations))\n",
    "\n",
    "for i in range(len(n_iterations)):\n",
    "    # Specifying the learning rate\n",
    "    iteration = n_iterations[i]\n",
    "\n",
    "    # Running the SGD Logistic Regression with the specified learning rate\n",
    "    SGD_test3 = OwnLogisticRegression(X_train, X_test, Y_train, Y_test, eta,iteration,10,lam)\n",
    "    SGD_test3.train(\"SGD\")\n",
    "    y_pred3 = SGD_test3.predict()\n",
    "    acc_test_SGD = accuracy_score(Y_test,y_pred3)\n",
    "    acc_array_SGD[i] = acc_test_SGD\n",
    "\n",
    "    # Running the GD Logistic Regression with the specified learning rate\n",
    "    GD_test3 = OwnLogisticRegression(X_train, X_test, Y_train, Y_test, eta,iteration,10,lam)\n",
    "    GD_test3.train(\"GD\")\n",
    "    y_pred4 = GD_test3.predict()\n",
    "    acc_test_GD = accuracy_score(Y_test,y_pred4)\n",
    "    acc_array_GD[i] = acc_test_GD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting the results versus learning rates'''\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataFrames for each method\n",
    "df_SGD = pd.DataFrame({\"Number of epochs\": n_iterations, 'Accuracy, %': acc_array_SGD, 'Method': 'Stochastic Gradient Descent'})\n",
    "df_GD = pd.DataFrame({\"Number of epochs\": n_iterations, 'Accuracy, %': acc_array_GD, 'Method': 'Gradient Descent'})\n",
    "#df_Scikit = pd.DataFrame({\"Number of epochs\": n_iterations, 'Accuracy, %': acc_array_Scikit, 'Method': 'Scikit-Learn'})\n",
    "\n",
    "# Concatenate DataFrames\n",
    "Accuracy_data = pd.concat([df_SGD, df_GD])\n",
    "\n",
    "# Set Seaborn style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set Matplotlib font family to Times New Roman\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "sns.lineplot(x='Number of epochs',y='Accuracy, %', hue='Method', data=Accuracy_data)\n",
    "\n",
    "# Set titles and labels\n",
    "plt.xlabel(\"Number of Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy Score\", fontsize=14)\n",
    "\n",
    "# Set legend\n",
    "plt.legend(title='Method', title_fontsize='14', fontsize='12')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Printing the Accuracy score for the different methods'''\n",
    "\n",
    "# Fixing parameters according to earlier tuning tests\n",
    "lam = 0.001\n",
    "eta = 0.01\n",
    "\n",
    "# Setting number of iterations to 1000\n",
    "n_iterations = 600\n",
    "\n",
    "# Calculating accuracy for Gradient Descent with optimal parameters for eta and lambda2\n",
    "GD_Test2 = OwnLogisticRegression(X_train, X_test, Y_train, Y_test, eta,n_iterations,10,lam)\n",
    "GD_Test2.train(\"GD\")\n",
    "y_pred_GD = GD_Test2.predict()\n",
    "accGD = accuracy_score(Y_test,y_pred_GD)\n",
    "\n",
    "# Calculating accuracy for Stochastic Gradient Descent with optimal parameters for eta and lambda2\n",
    "SGD_Test2 = OwnLogisticRegression(X_train, X_test, Y_train, Y_test, eta,n_iterations,10,lam)\n",
    "SGD_Test2.train(\"SGD\")\n",
    "y_pred_SGD = SGD_Test2.predict()\n",
    "accSGD = accuracy_score(Y_test,y_pred_SGD)\n",
    "\n",
    "# Calculating accuracy for Scikit-Learn with optimal parameters for eta and lambda2\n",
    "model_scikit = LogisticRegression(C=1/lam, max_iter=600, tol=eta)\n",
    "model_scikit.fit(X_train,Y_train)\n",
    "pred_sklearn = model_scikit.predict(X_test)\n",
    "accuracy_scikit = accuracy_score(Y_test,pred_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics using Scikit-Learn: \n",
      "-----------\n",
      "The accuracy using Scikit: 0.9473684210526315 \n",
      "\n",
      "The best accuracy with GD:  0.9736842105263158 \n",
      "\n",
      "The best accuracy with SGD:  0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "'''Comparing the results with Scikit-Learn'''\n",
    "\n",
    "print(\"Statistics using Scikit-Learn: \")\n",
    "print(\"-----------\")\n",
    "\n",
    "print(f\"The accuracy using Scikit:\",accuracy_scikit,'\\n')\n",
    "print(\"The best accuracy with GD: \",accGD,'\\n')\n",
    "print(\"The best accuracy with SGD: \",accSGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
